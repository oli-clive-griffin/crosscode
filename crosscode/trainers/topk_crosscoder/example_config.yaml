data:
  token_sequence_loader:
    sequence_length: 1024
  activations_harvester:
    llms:
      - name: gpt2
      - base_architecture_name: gpt2
        hf_model_name: submarat/model-without-ln
    harvesting_batch_size: 1
crosscoder:
  n_latents: 8192
  k: 32
train:
  topk_style: "batch_topk"
  num_steps: 100
  batch_size: 32
  log_every_n_steps: 10
  lambda_aux: 0.1
experiment_name: "topk_crosscoder_example"
hookpoints: [
  "blocks.4.hook_resid_post",
  "blocks.8.hook_resid_post",
]