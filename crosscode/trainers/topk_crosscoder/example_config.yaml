data:
  activations_harvester:
    llms:
      - name: EleutherAI/pythia-160M
        # revision: step142000
    harvesting_batch_size: 1
crosscoder:
  n_latents: 8192
  k: 64
train:
  topk_style: "topk"
  num_steps: 10_000
  batch_size: 16
  log_every_n_steps: 10
  lambda_aux: 0.1
experiment_name: "gemma_rich_tc"
hookpoints: [
  "blocks.6.ln2.hook_normalized",
  "blocks.6.hook_mlp_out",
]