data:
  activations_harvester:
    llms:
      - name: EleutherAI/pythia-160M
        revision: step143000
    harvesting_batch_size: 1
transcoder:
  n_latents: 8192
  k: 32
train:
  topk_style: "topk"
  num_steps: 100
  batch_size: 32
  log_every_n_steps: 10
experiment_name: topk_cross_layer_transcoder_example
in_hookpoint: "blocks.7.mlp.hook_pre"
out_hookpoints: [
  "blocks.7.mlp.hook_post",
  "blocks.8.mlp.hook_post",
]
